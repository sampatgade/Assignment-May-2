{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3481479c-f33c-4f9a-afd2-a5cae1a92bd0",
   "metadata": {},
   "source": [
    "Ans 1) \n",
    "Anomaly detection, also known as outlier detection, is a data analysis technique used to identify patterns or data points that significantly deviate from the norm or expected behavior within a dataset. The primary purpose of anomaly detection is to discover rare or unusual instances, events, or patterns that do not conform to the expected behavior or distribution within a dataset. Here's a more detailed explanation of anomaly detection and its purpose:\n",
    "\n",
    "Detecting Unusual Events or Data Points:\n",
    "\n",
    "Anomaly detection aims to find data points or events that are different from the majority of the data. These anomalies can take various forms, such as outliers, errors, fraud, defects, or unusual patterns.\n",
    "\n",
    "Anomalies are often of interest because they may indicate critical issues, opportunities, or problems that require further investigation or action.\n",
    "\n",
    "Applications of Anomaly Detection:\n",
    "\n",
    "Anomaly detection has a wide range of practical applications across different domains, including:\n",
    "\n",
    "Fraud Detection: Identifying fraudulent transactions or activities in financial systems, credit card transactions, or insurance claims.\n",
    "\n",
    "Network Security: Detecting unusual or suspicious network traffic that may signify cyberattacks or security breaches.\n",
    "\n",
    "Manufacturing: Identifying defective products or equipment failures in manufacturing processes to maintain product quality.\n",
    "\n",
    "Healthcare: Detecting anomalies in patient data for early disease diagnosis or monitoring patient health.\n",
    "\n",
    "Predictive Maintenance: Identifying equipment or machinery failures before they occur to reduce downtime and maintenance costs.\n",
    "\n",
    "Environmental Monitoring: Detecting abnormal environmental conditions, such as pollution spikes or unusual weather patterns.\n",
    "\n",
    "Image and Video Analysis: Detecting anomalies in images or video streams, useful in surveillance and quality control.\n",
    "\n",
    "Natural Language Processing: Identifying unusual or potentially fraudulent text data, such as fake reviews or spam emails.\n",
    "\n",
    "Benefits of Anomaly Detection:\n",
    "\n",
    "Early Detection: Anomaly detection helps identify issues or opportunities early, allowing for timely intervention.\n",
    "\n",
    "Cost Reduction: By detecting anomalies and addressing them promptly, organizations can save money by preventing fraud, reducing maintenance costs, and minimizing downtime.\n",
    "\n",
    "Enhanced Security: In cybersecurity and network monitoring, anomaly detection can help protect systems from threats and vulnerabilities.\n",
    "\n",
    "Quality Assurance: In manufacturing and healthcare, anomaly detection ensures that products meet quality standards and patient health is monitored effectively.\n",
    "\n",
    "Data Quality: Anomaly detection can improve data quality by identifying and rectifying errors in datasets.\n",
    "\n",
    "Challenges in Anomaly Detection:\n",
    "\n",
    "Determining What's Normal: Defining what constitutes normal behavior or patterns can be challenging, as it depends on the specific domain and context.\n",
    "\n",
    "Imbalanced Data: In some cases, anomalies are rare compared to normal data, leading to imbalanced datasets.\n",
    "\n",
    "False Positives: Anomaly detection systems can produce false alarms, requiring careful tuning to balance precision and recall.\n",
    "\n",
    "Adapting to Changing Environments: Anomaly detection models need to adapt to evolving data distributions and patterns.\n",
    "\n",
    "In summary, anomaly detection is a critical data analysis technique used to identify outliers and unusual patterns within datasets. Its primary purpose is to enhance decision-making, improve system reliability, and mitigate risks across various domains by identifying and addressing anomalies that might otherwise go unnoticed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195855a-c9f7-457a-ba5c-d7dbfdbe3f5c",
   "metadata": {},
   "source": [
    "Ans 2) \n",
    "Anomaly detection is a valuable technique, but it comes with several challenges that practitioners need to address to build effective anomaly detection systems. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "Defining \"Normal\" Behavior:\n",
    "\n",
    "One of the fundamental challenges is defining what constitutes normal behavior or patterns within a dataset. This definition is often context-dependent and can be subjective. What's considered normal in one context may not be in another.\n",
    "Imbalanced Data:\n",
    "\n",
    "Anomalies are typically rare compared to normal data points. This class imbalance can lead to models that are biased toward predicting normal instances, resulting in poor anomaly detection performance.\n",
    "Labeling Anomalies:\n",
    "\n",
    "In many real-world applications, labeled data with known anomalies may be scarce or unavailable. Labeling anomalies for training and evaluation purposes can be costly and time-consuming.\n",
    "Scalability:\n",
    "\n",
    "As data volumes grow, the scalability of anomaly detection methods becomes a challenge. Some algorithms may not scale well to large datasets or high-dimensional feature spaces.\n",
    "Data Quality and Noise:\n",
    "\n",
    "Noisy data, missing values, or data errors can affect the accuracy of anomaly detection algorithms. Cleaning and preprocessing data are essential steps to mitigate these issues.\n",
    "Temporal and Spatial Dependencies:\n",
    "\n",
    "In time series data or spatial data, anomalies can exhibit dependencies over time or space. Detecting anomalies while considering these dependencies is more complex than simple point-based anomalies.\n",
    "Concept Drift:\n",
    "\n",
    "Anomaly detection models assume that the underlying data distribution remains stable. In reality, data distributions can change over time due to various factors, leading to the concept drift problem.\n",
    "Scarcity of Anomalies:\n",
    "\n",
    "In some cases, anomalies are exceptionally rare, making it challenging to collect enough labeled data to train effective models or to set appropriate thresholds.\n",
    "Feature Engineering:\n",
    "\n",
    "Choosing the right features or representations of data is crucial for effective anomaly detection. Poor feature selection can lead to suboptimal results.\n",
    "Model Selection:\n",
    "\n",
    "Selecting the right anomaly detection algorithm or model for a specific problem can be challenging. Different algorithms may perform better in different contexts.\n",
    "Threshold Setting:\n",
    "\n",
    "Determining the appropriate threshold for classifying data points as anomalies or normal can be difficult. A threshold that is too low may result in too many false positives, while a threshold that is too high may miss genuine anomalies.\n",
    "Interpreting Anomalies:\n",
    "\n",
    "Identifying the root causes or explanations for detected anomalies can be challenging. Simply flagging anomalies without understanding their context may limit the usefulness of the detection.\n",
    "Evolving Anomalies:\n",
    "\n",
    "Anomalies may evolve and adapt to the detection methods used. Anomaly detection systems need to adapt to new types of anomalies or changing attack patterns.\n",
    "Privacy Concerns:\n",
    "\n",
    "In some applications, the data being analyzed may contain sensitive or private information. Balancing the need for anomaly detection with privacy concerns can be a challenge.\n",
    "False Positives and False Negatives:\n",
    "\n",
    "Finding the right trade-off between minimizing false positives (normal data misclassified as anomalies) and false negatives (anomalies missed) is a common challenge in anomaly detection.\n",
    "Addressing these challenges often requires a combination of domain knowledge, data preprocessing, algorithm selection, and ongoing monitoring and adaptation of the anomaly detection system. Additionally, the choice of methodology and approach should align with the specific characteristics and requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d4397-8b25-445f-a535-e67887ec6fa5",
   "metadata": {},
   "source": [
    "Ans 3) Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches used to identify and classify anomalies within a dataset. They differ primarily in how they leverage labeled data and the level of human supervision involved:\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "\n",
    "Lack of Labeled Data:\n",
    "\n",
    "In unsupervised anomaly detection, you typically don't have labeled data, meaning you don't know in advance which data points are normal and which are anomalies.\n",
    "The algorithm's task is to identify anomalies solely based on the inherent structure and characteristics of the data.\n",
    "Algorithmic Approach:\n",
    "\n",
    "Unsupervised methods focus on identifying data points that deviate significantly from the majority of the data or follow an unusual pattern.\n",
    "Common unsupervised techniques include clustering-based approaches (e.g., K-means, DBSCAN), density-based methods (e.g., Local Outlier Factor), and dimensionality reduction (e.g., Principal Component Analysis) for anomaly detection.\n",
    "Applications:\n",
    "\n",
    "Unsupervised anomaly detection is valuable when dealing with data where anomalies are rare and the cost of labeling anomalies is prohibitive. It's used in various domains, including fraud detection, network security, and manufacturing quality control.\n",
    "Challenges:\n",
    "\n",
    "Defining what constitutes normal behavior can be challenging without labeled data.\n",
    "It may produce false positives and false negatives, and tuning the algorithm for a specific problem can be tricky.\n",
    "Supervised Anomaly Detection:\n",
    "\n",
    "Availability of Labeled Data:\n",
    "\n",
    "In supervised anomaly detection, you have a labeled dataset, where anomalies are explicitly labeled as such. You know which data points are normal and which are anomalies.\n",
    "Classification Approach:\n",
    "\n",
    "Supervised methods treat anomaly detection as a classification problem. You build a machine learning model using the labeled data to classify new data points as normal or anomalies.\n",
    "Common supervised algorithms include decision trees, support vector machines, and deep learning models.\n",
    "Applications:\n",
    "\n",
    "Supervised anomaly detection is suitable when you have a labeled dataset, making it easier to build a classification model. It's used in applications such as email spam detection, medical diagnosis, and industrial fault detection.\n",
    "Benefits:\n",
    "\n",
    "It typically results in better accuracy compared to unsupervised methods when you have labeled data for training.\n",
    "It provides clear class labels for anomalies, making it easier to interpret and act upon the results.\n",
    "Challenges:\n",
    "\n",
    "Supervised anomaly detection relies on having a labeled dataset, which may not always be available or may be expensive to obtain.\n",
    "It assumes that the labeled anomalies in the training set represent all possible anomalies, which may not hold in some cases.\n",
    "In summary, the main difference between unsupervised and supervised anomaly detection lies in the availability of labeled data and the approach used to detect anomalies. Unsupervised methods operate without labeled data, while supervised methods leverage labeled data to build classification models for anomaly detection. The choice between these approaches depends on the availability of labeled data and the specific requirements of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf5f34-1cd8-42f5-8292-eb17165479bf",
   "metadata": {},
   "source": [
    "Ans 4)\n",
    "Anomaly detection algorithms can be categorized into several main categories based on their underlying approaches and techniques. These categories encompass various methods for identifying anomalies within datasets. The main categories of anomaly detection algorithms include:\n",
    "\n",
    "Statistical Methods:\n",
    "\n",
    "Statistical methods assume that normal data points follow a certain statistical distribution (e.g., Gaussian distribution) and identify anomalies as data points that significantly deviate from this distribution.\n",
    "Common statistical methods include z-score, modified z-score, and the Grubbs' test.\n",
    "Machine Learning-Based Methods:\n",
    "\n",
    "Machine learning-based approaches involve training models on labeled or unlabeled data to distinguish between normal and anomalous instances.\n",
    "Supervised learning methods, such as decision trees, support vector machines (SVMs), and neural networks, can be adapted for anomaly detection when labeled data is available.\n",
    "Unsupervised learning methods, like clustering (e.g., K-means) and dimensionality reduction (e.g., PCA), can also be used for anomaly detection.\n",
    "Nearest Neighbor-Based Methods:\n",
    "\n",
    "These methods identify anomalies based on the distance or similarity between data points. Anomalies are typically distant from their nearest neighbors.\n",
    "Examples include k-nearest neighbors (KNN) and local outlier factor (LOF).\n",
    "Density-Based Methods:\n",
    "\n",
    "Density-based methods identify anomalies by assessing the density of data points in different regions of the feature space. Anomalies are often located in low-density areas.\n",
    "Density-based clustering algorithms like DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can be used for anomaly detection.\n",
    "Clustering-Based Methods:\n",
    "\n",
    "Clustering-based methods group similar data points together and treat outliers as data points that do not belong to any cluster or belong to small clusters.\n",
    "K-means clustering can be used for this purpose, as well as other clustering algorithms.\n",
    "Dimensionality Reduction Methods:\n",
    "\n",
    "Dimensionality reduction techniques, such as Principal Component Analysis (PCA) and autoencoders, can be used to reduce the dimensionality of data while preserving important information. Anomalies may be more apparent in the reduced-dimensional space.\n",
    "Spectral Methods:\n",
    "\n",
    "Spectral methods leverage eigenvalues and eigenvectors of matrices derived from data to identify anomalies.\n",
    "One example is the use of spectral clustering for anomaly detection.\n",
    "Ensemble Methods:\n",
    "\n",
    "Ensemble methods combine multiple models or algorithms to improve the overall performance of anomaly detection.\n",
    "Examples include Isolation Forest, Random Forest, and ensemble-based voting schemes.\n",
    "One-Class Classification:\n",
    "\n",
    "In this approach, a model is trained on normal data only and is then used to classify data points as either normal or anomalous.\n",
    "One-Class SVM is a popular technique in this category.\n",
    "Deep Learning Methods:\n",
    "\n",
    "Deep learning models, such as autoencoders and deep neural networks, can learn complex representations of data and detect anomalies based on reconstruction errors or model uncertainty.\n",
    "Time-Series Anomaly Detection:\n",
    "\n",
    "Specialized methods for detecting anomalies in time-series data, where the temporal order of data points is important.\n",
    "Techniques include moving averages, exponential smoothing, and methods based on autoregressive models.\n",
    "These categories represent different strategies and approaches for identifying anomalies in data. The choice of the most suitable algorithm or method depends on the specific characteristics of the data, the nature of the anomalies, and the requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44f555-7880-42dc-9b3c-83cd3a0ba8d1",
   "metadata": {},
   "source": [
    "Ans 5) \n",
    "Distance-based anomaly detection methods make several key assumptions about the underlying data and the nature of anomalies. These assumptions are important to consider when applying these methods, as they can impact the effectiveness of the anomaly detection process. The main assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "Assumption of Normality:\n",
    "\n",
    "Distance-based methods often assume that the normal data points in the dataset follow a particular probability distribution, such as a Gaussian (normal) distribution.\n",
    "Anomalies are expected to deviate significantly from this assumed distribution.\n",
    "Euclidean Distance Metric:\n",
    "\n",
    "Many distance-based methods use the Euclidean distance metric to measure the proximity or similarity between data points.\n",
    "This assumes that the data is represented in a continuous feature space, and the relationships between features are linear and isotropic (uniform in all directions).\n",
    "Homogeneity of Clusters:\n",
    "\n",
    "Distance-based methods may assume that normal data points are clustered together in dense groups, while anomalies are isolated or far from these clusters.\n",
    "Anomalies are expected to be less homogeneously distributed.\n",
    "Noisy Data:\n",
    "\n",
    "These methods often assume that the data contains some level of noise, and anomalies are considered data points that are not explained by the underlying noise or are outliers relative to the noise level.\n",
    "Single Global Model:\n",
    "\n",
    "Some distance-based methods assume the existence of a single global model or reference distribution for normal data.\n",
    "Anomalies are identified as data points that are inconsistent with this global model.\n",
    "Constant Density:\n",
    "\n",
    "Certain methods assume that the density of normal data points remains relatively constant throughout the feature space.\n",
    "Anomalies are those data points found in regions with significantly lower density.\n",
    "Independence of Features:\n",
    "\n",
    "Many distance-based methods assume that the features used for measuring distances are independent of each other.\n",
    "Dependencies or correlations between features may not be fully considered, potentially leading to limitations when handling high-dimensional data.\n",
    "Noisy Labels:\n",
    "\n",
    "In the case of labeled data, distance-based methods may assume that the labels are accurate and reliable. However, noisy labels can impact the quality of training and evaluation.\n",
    "It's important to note that these assumptions may not always hold in real-world scenarios. Depending on the characteristics of the data and the nature of anomalies, some of these assumptions may be violated, leading to limitations in the effectiveness of distance-based anomaly detection methods. As a result, it's essential to carefully assess the suitability of these methods for a specific problem and dataset and to consider alternative approaches when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2facea60-4eae-4b74-aedd-52bd3da492c9",
   "metadata": {},
   "source": [
    "Ans 6) The Local Outlier Factor (LOF) algorithm computes anomaly scores for data points based on their local density compared to the local densities of their neighbors. LOF is a density-based anomaly detection method that quantifies how much more or less dense a data point is compared to its neighbors. The steps for computing anomaly scores with the LOF algorithm are as follows:\n",
    "\n",
    "Define the Nearest Neighbors:\n",
    "\n",
    "Choose a parameter, k, that represents the number of nearest neighbors to consider. k is a hyperparameter that you need to specify in advance.\n",
    "For each data point, find its k nearest neighbors based on a distance metric, typically Euclidean distance.\n",
    "Compute Reachability Distance:\n",
    "\n",
    "For each data point, compute its reachability distance with respect to its k nearest neighbors. The reachability distance measures how far a data point is from its neighbors.\n",
    "\n",
    "The reachability distance, denoted as RD(p, o) for a data point p and its neighbor o, is calculated as follows:\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "RD(p, o) = max{distance(p, o), k-distance(o)}\n",
    "distance(p, o) is the Euclidean distance between data point p and its neighbor o.\n",
    "\n",
    "k-distance(o) is the distance from the neighbor o to its k-th nearest neighbor.\n",
    "\n",
    "Compute Local Reachability Density (LRD):\n",
    "\n",
    "For each data point, calculate its local reachability density (LRD), which is a measure of how dense the data point is relative to its neighbors.\n",
    "\n",
    "LRD is computed as the inverse of the average reachability distance of a data point's k nearest neighbors:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "LRD(p) = 1 / (sum(RD(p, o) for o in k-nearest neighbors of p) / k)\n",
    "Compute Local Outlier Factor (LOF):\n",
    "\n",
    "Finally, compute the local outlier factor (LOF) for each data point, which quantifies how much of an outlier the data point is compared to its neighbors.\n",
    "\n",
    "LOF is calculated as the average LRD of a data point's k nearest neighbors divided by its own LRD:\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "LOF(p) = (sum(LRD(o) for o in k-nearest neighbors of p) / k) / LRD(p)\n",
    "Anomaly Score:\n",
    "\n",
    "The LOF values obtained represent the anomaly scores for each data point. Higher LOF values indicate that a data point is more likely to be an outlier, as it has a significantly different density compared to its neighbors.\n",
    "In summary, the LOF algorithm computes anomaly scores based on the local densities of data points and their relationships with their neighbors. Data points with higher LOF scores are considered more likely to be anomalies, as they have lower local densities relative to their neighbors. The LOF algorithm is effective in detecting anomalies that have a different local density pattern compared to the majority of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609ffdf-ee38-4c5f-b2dd-03dad9030998",
   "metadata": {},
   "source": [
    "Ans 7) \n",
    "The Isolation Forest algorithm has a few key parameters that you can tune to control its behavior and performance. These parameters influence how the algorithm constructs the isolation trees and determines anomaly scores. The main parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "n_estimators:\n",
    "\n",
    "This parameter specifies the number of isolation trees to build in the forest. More trees can provide better accuracy but may also increase computation time.\n",
    "Increasing the number of trees generally improves the model's ability to detect anomalies but can lead to diminishing returns.\n",
    "max_samples:\n",
    "\n",
    "It determines the maximum number of data points to be used when building each isolation tree.\n",
    "Smaller values result in more random sampling of data points, making the trees more diverse but potentially less accurate.\n",
    "Larger values make the trees more deterministic and use more data points for each tree.\n",
    "max_features:\n",
    "\n",
    "This parameter controls the maximum number of features (attributes) to consider when splitting nodes in the tree.\n",
    "Setting it to a smaller value introduces more randomness and diversity among the trees.\n",
    "A larger value, such as the total number of features, makes the trees less diverse.\n",
    "contamination:\n",
    "\n",
    "Contamination is a critical parameter that determines the proportion of anomalies in the dataset. It reflects the expected fraction of anomalies in the data.\n",
    "You can set it to a specific float value (e.g., 0.1 for 10% anomalies) or use the string 'auto', which estimates the contamination based on the training data.\n",
    "The threshold for classifying a data point as an anomaly is determined by this parameter.\n",
    "random_state:\n",
    "\n",
    "This parameter controls the random seed for reproducibility. By setting a specific random state, you ensure that the same results are obtained each time you run the algorithm with the same parameters and data.\n",
    "n_jobs:\n",
    "\n",
    "It specifies the number of CPU cores to use for parallelization during tree construction.\n",
    "Setting n_jobs to -1 uses all available CPU cores, potentially speeding up the training process.\n",
    "These parameters allow you to fine-tune the Isolation Forest algorithm to suit your specific anomaly detection task. The choice of parameter values should be made based on the characteristics of your data and the trade-off between computation time and detection performance. Experimenting with different parameter combinations and evaluating the results on validation data is often necessary to determine the optimal settings for your particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7d52f-28bf-4327-8ad9-3a4336613a54",
   "metadata": {},
   "source": [
    "Ans 8) To compute the anomaly score of a data point using K-Nearest Neighbors (KNN) with K=10, and given that the data point has only 2 neighbors of the same class within a radius of 0.5, you can follow these steps:\n",
    "\n",
    "Calculate the Reachability Distance (RD) for the data point:\n",
    "\n",
    "For each of its neighbors, calculate the reachability distance as the maximum of the distance between the data point and the neighbor or the distance to the 10th nearest neighbor of the neighbor.\n",
    "In this case, you have two neighbors, so calculate the reachability distance for each neighbor.\n",
    "Compute the Local Reachability Density (LRD) for the data point:\n",
    "\n",
    "The LRD for the data point is the inverse of the average reachability distance of its neighbors.\n",
    "Calculate the Local Outlier Factor (LOF) for the data point:\n",
    "\n",
    "The LOF for the data point is computed as the ratio of the average LRD of its neighbors to its own LRD.\n",
    "Let's assume that the data point is denoted as \"P,\" and it has two neighbors (N1 and N2) of the same class within a radius of 0.5. We'll calculate the anomaly score step by step:\n",
    "\n",
    "RD(N1, P): Calculate the reachability distance between N1 and P.\n",
    "\n",
    "RD(N2, P): Calculate the reachability distance between N2 and P.\n",
    "\n",
    "Average RD(P) = (RD(N1, P) + RD(N2, P)) / 2\n",
    "\n",
    "LRD(P): Calculate the local reachability density for P, which is the inverse of the average RD(P).\n",
    "\n",
    "LOF(P): Calculate the local outlier factor for P, which is the ratio of the average LRD of its neighbors to its own LRD.\n",
    "\n",
    "The LOF(P) value will indicate the anomaly score for the data point P. If LOF(P) is significantly higher than 1, it suggests that P is an outlier or anomaly relative to its neighbors. The specific LOF value will depend on the distances and density measurements in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcaa3c3-9277-4e19-bcc6-4b8e73574b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/neighbors/_lof.py:282: UserWarning: n_neighbors (10) is greater than the total number of samples (5). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LocalOutlierFactor' object has no attribute '_decision_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m lof\u001b[38;5;241m.\u001b[39mfit(data_points)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate the LOF score for the data point P\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m lof_score_P \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mlof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m([P])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOF Score for P:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lof_score_P)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LocalOutlierFactor' object has no attribute '_decision_function'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Sample data points\n",
    "data_points = [[1.0, 2.0], [1.2, 2.1], [3.0, 3.0], [3.2, 3.1], [4.0, 4.0]]\n",
    "\n",
    "# Data point P with two neighbors within a radius of 0.5\n",
    "P = [1.5, 2.5]\n",
    "N1 = [1.0, 2.0]\n",
    "N2 = [1.2, 2.1]\n",
    "\n",
    "# Create an instance of the LOF algorithm\n",
    "lof = LocalOutlierFactor(n_neighbors=10, contamination='auto')\n",
    "\n",
    "# Fit the LOF model on the sample data\n",
    "lof.fit(data_points)\n",
    "\n",
    "# Calculate the LOF score for the data point P\n",
    "lof_score_P = -lof._decision_function([P])[0]\n",
    "\n",
    "print(\"LOF Score for P:\", lof_score_P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea4ae1-e560-4a81-8755-e003a9b4f2e8",
   "metadata": {},
   "source": [
    "Ans 9 ) In the Isolation Forest algorithm, the anomaly score for a data point is typically computed based on its average path length in the ensemble of isolation trees relative to the average path length of all data points in the dataset. The average path length of a data point in the isolation trees is used to quantify how isolated or easy to isolate that data point is.\n",
    "\n",
    "Given that you have 100 trees and a dataset of 3000 data points, and you want to calculate the anomaly score for a data point with an average path length of 5.0 compared to the average path length of all data points, you can use the following formula:\n",
    "\n",
    "Anomaly Score = 2^(-average path length / c(n))\n",
    "\n",
    "Where:\n",
    "\n",
    "average path length is the average path length of the data point in the isolation trees (in this case, 5.0).\n",
    "c(n) is a constant that depends on the number of data points in the dataset (in this case, 3000).\n",
    "The constant c(n) can be calculated as follows:\n",
    "\n",
    "c(n) = 2 * (log(n-1) + 0.5772156649) - (2 * (n-1) / n)\n",
    "\n",
    "Where:\n",
    "\n",
    "n is the number of data points in the dataset (3000 in this case).\n",
    "Let's calculate the anomaly score:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import math\n",
    "\n",
    "# Given values\n",
    "average_path_length = 5.0\n",
    "n = 3000\n",
    "\n",
    "# Calculate the constant c(n)\n",
    "c_n = 2 * (math.log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)\n",
    "\n",
    "# Calculate the anomaly score\n",
    "anomaly_score = 2 ** (-average_path_length / c_n)\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)\n",
    "Now, you can use this code to compute the anomaly score for the data point with an average path length of 5.0 compared to the average path length of the trees in your Isolation Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac6f76-eb03-4b01-9dd7-c1eb7008c189",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
